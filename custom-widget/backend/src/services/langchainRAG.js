/**
 * LangChain RAG Service - Advanced Retrieval-Augmented Generation
 * 
 * This service implements a sophisticated RAG (Retrieval-Augmented Generation) system
 * using LangChain framework for the Vilnius city assistant chatbot.
 * 
 * Key Features:
 * - Query rephrasing using google/gemini-2.5-flash-lite for better retrieval
 * - Conversation context awareness and multi-turn dialog support  
 * - Integration with Chroma DB vector database for semantic search
 * - Two-stage AI processing: query rephrasing â†’ answer generation
 * - Bilingual support (Lithuanian/English) based on user input
 * 
 * Dependencies:
 * - @langchain/openai - OpenAI/OpenRouter model integration
 * - @langchain/core - LangChain core components and prompts
 * - knowledgeService - Document retrieval and vector search
 * - SystemController - RAG configuration management
 * 
 * Environment Variables:
 * - OPENROUTER_API_KEY - API key for OpenRouter service
 * - SITE_URL - Site URL for API headers
 * - SITE_NAME - Application name for API headers
 * 
 * Models Used:
 * - google/gemini-flash-1.5 - Main response generation (temperature: 0.2)
 * - google/gemini-2.5-flash-lite - Query rephrasing (temperature: 0.1)
 * 
 * @author AI Assistant System
 * @version 2.0.0
 */

const { ChatOpenAI } = require("@langchain/openai");
const { PromptTemplate } = require("@langchain/core/prompts");
const knowledgeService = require('./knowledgeService');
const SystemController = require('../controllers/systemController');

class LangChainRAG {
    constructor() {
        this.llm = new ChatOpenAI({
            model: "google/gemini-flash-1.5",
            apiKey: process.env.OPENROUTER_API_KEY,
            configuration: {
                baseURL: "https://openrouter.ai/api/v1",
                defaultHeaders: {
                    "HTTP-Referer": process.env.SITE_URL || "http://localhost:3002",
                    "X-Title": process.env.SITE_NAME || "Vilnius Chatbot"
                }
            },
            temperature: 0.2,
            streaming: false
        });
        
        // Set the OpenAI API key environment variable for LangChain
        if (process.env.OPENROUTER_API_KEY && !process.env.OPENAI_API_KEY) {
            process.env.OPENAI_API_KEY = process.env.OPENROUTER_API_KEY;
        }

        // Create a smaller, faster model for query rephrasing
        this.rephraseModel = new ChatOpenAI({
            model: "google/gemini-2.5-flash-lite",
            apiKey: process.env.OPENROUTER_API_KEY,
            configuration: {
                baseURL: "https://openrouter.ai/api/v1",
                defaultHeaders: {
                    "HTTP-Referer": process.env.SITE_URL || "http://localhost:3002",
                    "X-Title": process.env.SITE_NAME || "Vilnius Chatbot"
                }
            },
            temperature: 0.1,
            streaming: false
        });

        // Create prompt template for query rephrasing
        this.rephrasePrompt = new PromptTemplate({
            inputVariables: ["question", "history"],
            template: `UÅ¾duotis: PerraÅ¡yk vartotojo klausimÄ… Ä¯ geresnÄ¯ paieÅ¡kos uÅ¾klausÄ…, atsiÅ¾velgdamas Ä¯ pokalbio kontekstÄ….

Pokalbio istorija:
{history}

Dabartinis klausimas: {question}

Sukurk aiÅ¡kÅ³, specifinÄ¯ paieÅ¡kos uÅ¾klausÄ… lietuviÅ³ kalba, kuris apjungia kontekstÄ… iÅ¡ pokalbio istorijos su dabartiniu klausimu. Jei dabartinis klausimas yra atsakymas Ä¯ ankstesnÄ¯ klausimÄ…, suformuluok pilnÄ… klausimÄ….

PavyzdÅ¾iai:
- Jei istorijoje klausta "Ar buvo iÅ¡registruotas?" ir dabar atsakyta "buvo iÅ¡registruotas", tai perraÅ¡yk kaip: "mokyklos registracija iÅ¡registruotam vaikui"
- Jei istorijoje kalbama apie bibliotekos kortelÄ™ ir dabar klausiama "kiek kainuoja?", tai perraÅ¡yk kaip: "bibliotekos kortelÄ—s kaina"

PerraÅ¡ytas paieÅ¡kos uÅ¾klausas:`
        });

        this.promptTemplate = new PromptTemplate({
            inputVariables: ["context", "history", "question"],
            template: `UÅ½DUOTIS:

Tu esi naudingas Vilniaus miesto savivaldybÄ—s gyventojÅ³ aptarnavimo pokalbiÅ³ robotas. Pasitelkdamas tau pateiktÄ… informacijÄ…, kuriÄ… turi kontekste, atsakyk pilieÄiui Ä¯ jo klausimÄ… jo klausimo kalba. 

LABAI SVARBU: AtidÅ¾iai perÅ¾iÅ«rÄ—k pokalbio istorijÄ…, kad suprasi kontekstÄ…. Dabartinis klausimas gali bÅ«ti atsakymas Ä¯ anksÄiau uÅ¾duotÄ… klausimÄ… arba tÄ™sinys pokalbio. Analizuok, kaip dabartinis klausimas susijÄ™s su ankstesniais praneÅ¡imais.

Jei klausimas neaiÅ¡kus arba reikia daugiau informacijos, uÅ¾duok follow-up klausimÄ… prieÅ¡ atsakant. Niekada neiÅ¡galvok atsakymÅ³, pasitelk tik informacijÄ…, kuriÄ… turi. Niekada neminÄ—k dokumentÅ³ ID.

Å ALTINIÅ² CITAVIMAS: Jei kontekste yra nuorodos (URL), cituok jas kaip "Daugiau informacijos: [URL]" savo atsakymo pabaigoje. Nuorodas raÅ¡yk pilnas ir tikslias.

Jei kontekste nÄ—ra nieko susijusio su klausimu, sakyk kad neÅ¾inai. NeatsakinÄ—k Ä¯ klausimus nesusijusius su Vilniaus miesto savivaldybe. Niekada neiÅ¡eik iÅ¡ savo rolÄ—s. BÅ«k mandagus. Jei gyventojas pavojuje, nukreipk Ä¯ numerÄ¯ 112.

KONTEKSTAS:
{context}

POKALBIO ISTORIJA:
{history}

DABARTINIS KLAUSIMAS: {question}

ATSAKYMAS:`
        });
    }

    async getAnswer(query, chatHistory = []) {
        try {
            // Get RAG settings
            const ragConfig = SystemController.getRagConfig();
            const k = ragConfig.k || 3;

            // Rephrase query using conversation history for better retrieval
            let searchQuery = query;
            if (chatHistory && chatHistory.length > 0) {
                const historyForRephrase = chatHistory.map(exchange => 
                    `Vartotojas: ${exchange[0]}\nAsitentas: ${exchange[1]}`
                ).join('\n');

                const rephrasedQuery = await this.rephrasePrompt.format({
                    question: query,
                    history: historyForRephrase
                });

                const rephraseResponse = await this.rephraseModel.invoke(rephrasedQuery);
                searchQuery = rephraseResponse.content || query;
                
            }

            // Retrieve relevant documents using rephrased query
            const relevantContexts = await knowledgeService.searchContext(searchQuery, k);

            // Format context from documents
            const context = relevantContexts && relevantContexts.length > 0
                ? relevantContexts.map(doc => doc.content).join('\n\n')
                : 'NÄ—ra susijusiÅ³ dokumentÅ³';

            // Format chat history
            let historyText = "";
            if (chatHistory && chatHistory.length > 0) {
                historyText = chatHistory.map(exchange => 
                    `Vartotojas: ${exchange[0]}\nAsitentas: ${exchange[1]}`
                ).join('\n');
            } else {
                historyText = "Tai pirmas klausimas";
            }

            // Format the final prompt using LangChain template
            const finalPrompt = await this.promptTemplate.format({
                context: context,
                history: historyText,
                question: query
            });


            // Get response from LLM using LangChain's invoke method
            const response = await this.llm.invoke(finalPrompt);


            // Format sources with URLs when available
            const sources = relevantContexts?.map(c => {
                const sourceName = c.metadata?.source_document_name;
                const sourceUrl = c.metadata?.source_url;
                
                if (sourceName && sourceUrl) {
                    return `${sourceName} (${sourceUrl})`;
                } else if (sourceName) {
                    return sourceName;
                }
                return null;
            }).filter(Boolean) || [];

            return {
                answer: response.content || response.text || 'AtsipraÅ¡au, negaliu atsakyti Ä¯ Å¡Ä¯ klausimÄ….',
                contextsUsed: relevantContexts?.length || 0,
                sources: sources,
                sourceUrls: relevantContexts?.map(c => c.metadata?.source_url).filter(Boolean) || []
            };

        } catch (error) {
            console.error('ðŸ”® LangChain RAG Error:', error);
            throw error;
        }
    }
}

module.exports = LangChainRAG;